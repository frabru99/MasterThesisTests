from logging import config, getLogger
from logging_config import TEST_LOGGING_CONFIG
config.dictConfig(TEST_LOGGING_CONFIG)
logger = getLogger(__name__)

import onnx
import torch
import torch_pruning as tp
import torch.nn as nn
import torch.nn.utils.prune as prune
import torch.quantization as tq
from abc import ABC, abstractmethod
from pathlib import Path
from copy import deepcopy
from torch.quantization import quantize_fx
from onnxruntime.tools.symbolic_shape_infer import SymbolicShapeInference
from onnxruntime import quantization
from onnxruntime.quantization import quantize_dynamic, QuantType
from onnxruntime.quantization.shape_inference import quant_pre_process
from BenchmarkingFactory.calibrationDataReader import CustomCalibrationDataReader
from BenchmarkingFactory.dataWrapper import DataWrapper
from BenchmarkingFactory.aiModel import AIModel

PROJECT_ROOT = Path(__file__).resolve().parent.parent

class Optimization(ABC): 

    @abstractmethod
    def applyOptimization(self):
        pass

    @abstractmethod
    def getOptimizationInfo(self, info: str):
        pass

class PruningOptimization(Optimization):

    def __init__(self, optimization_config):
        """
        Empty Constructor, load dynamically models to apply and create a copy of the new model
        """
        
        self.current_aimodel = None
        self.optimization_config = optimization_config

    def applyOptimization(self, input_loader):
        """
        Apply structural pruning using data from the loader for shape inference.
        
        Input:
            - input_loader: DataLoader to fetch a sample input batch (for shape detection)
        """ 

        logger.debug(f"-----> [OPTIMIZATION MODULE] APPLY STRUCTURAL PRUNING")

        if not self.current_aimodel:
            raise MissingAIModelError("No AIModel set.")

        current_model_info = self.current_aimodel.getAllInfo()
        pruned_model_info = deepcopy(current_model_info)
        pruned_aimodel = AIModel(pruned_model_info)
        model_to_prune = pruned_aimodel.getModel()

        method = self.getOptimizationInfo('method')
        device = self.current_aimodel.getInfo('device')
        amount = self.getOptimizationInfo('amount')
        n = self.getOptimizationInfo('n')

        try:
            batch = next(iter(input_loader))
            if isinstance(batch, (list, tuple)):
                example_inputs = batch[0]
            else:
                example_inputs = batch

            example_inputs = example_inputs.to(device)
            logger.debug(f"Shape inference using input size: {example_inputs.shape}")
        except Exception as e:
            logger.error(f"Failed to fetch batch from loader: {e}")

        # Define importance strategy
        imp = self.__getImportanceMethod(method)

        # Ignore last classifier
        ignored_layers = []
        for m in model_to_prune.modules():
            if isinstance(m, torch.nn.Linear) and m.out_features == self.current_aimodel.getInfo('num_classes'):
                ignored_layers.append(m)

        pruner = tp.pruner.MagnitudePruner(
            model_to_prune,
            example_inputs,
            importance=imp,
            iterative_steps=1,    
            ch_sparsity=amount,   
            ignored_layers=ignored_layers,
        )

        # Execute Pruning
        logger.info(f"Pruning model with structural pruning... Target Sparsity: {amount}")
        pruner.step() 
        
        # Check the result
        logger.debug(f"Model physically shrunk. New structure applied.")

        # Update info
        pruned_aimodel.getAllInfo()['model_name'] += "_pruned"
        pruned_aimodel.getAllInfo()['description'] += f"(Structurally Pruned amount {amount})"
        
        logger.debug(f"<----- [OPTIMIZATION MODULE] DONE")

        return pruned_aimodel

    def __getImportanceMethod(self, method: str):
        """
        Get the Importance method to use in the pruning

        Input:
            - method: string that defines the method

        Output:
            - importance_method: class used from torch_pruning
        """

        if method == "Random":
            logger.info(f"Using Random Strategy for Pruning")
            imp = tp.importance.RandomImportance()
        elif methdo == "LnStructured":
            logger.info(f"Using Ln Structuerd Strategy for Pruning")
            n = self.getOptimizationInfo('n')
            imp = tp.importance.MagnitudeImportance(p=n)

        return imp


    def setOptimizationConfig(self, optimization_config):
        """
        Set the Optimization Config to apply

        Input:
            -optimization_config: config given from doe experiment
        """
        self.optimization_config = optimization_config

    def setAIModel(self, AIModel):
        """
        Set an AIModel in ordert to apply on it the pruning optimization
        """
        self.current_aimodel =  AIModel

    def getOptimizationInfo(self, info: str):
        """
        Return the choosen info from the optimization config setted
        """
        return self.optimization_config[info]

  


class QuantizationOptimization(Optimization):

    def __init__(self, optimization_config):
        """
        Empty Constructor, load dynamically models to apply and create a copy of the new model
        """
        
        self.current_aimodel = None
        self.optimization_config = optimization_config

    def getOptimizationInfo(self, info: str):
        """
        Return the choosen info from the optimization config setted
        """
        return self.optimization_config[info]

    def setOptimizationConfig(self, optimization_config):
        """
        Set the Optimization Config to apply

        Input:
            -optimization_config: config given from doe experiment
        """
        self.optimization_config = optimization_config

    def setAIModel(self, AIModel):
        """
        Set an AIModel in ordert to apply on it the pruning optimization
        """
        self.current_aimodel =  AIModel

    def applyOptimization(self, input_examples):
        """
        Apply the optimization quantization configured with config, to the aiModel attached

        Input:
            -N.A.

        Output:
            -optimized_model: aiModel optimized with dynamic or static quantization
        """
        
        logger.debug(f"-----> [OPTIMIZATION MODULE] APPLY QUANTIZATION OPTIMIZATION")
    
        if not self.current_aimodel:
            raise MissingAIModelError(
                "No AIModel set. Call setAIModel() before applying optimization"
            )

        device = self.current_aimodel.getInfo('device')
        model_name = self.current_aimodel.getInfo('model_name')
        bit_method = self.getOptimizationInfo('method')
        quant_type = self.getOptimizationInfo('type')

        current_model_info = self.current_aimodel.getAllInfo()
        quantized_model_info = deepcopy(current_model_info)

        logger.debug(f"Creating a copy of the model {current_model_info['model_name']}")
        quantized_aimodel = AIModel(quantized_model_info)

        match quant_type:
            case "static":
                res = self.__staticQuantizationOnnx(model_name, device, input_examples)   
            
            case "dynamic":
                pass

        quantized_aimodel.getAllInfo()['model_name'] += "_quantized"
        quantized_aimodel.getAllInfo()['description'] += f"(Quantized with type:{self.getOptimizationInfo('type')} ; bit_method:{self.getOptimizationInfo('method')})"
        logger.debug(f"<----- [OPTIMIZATION MODULE] APPLY QUANTIZATION OPTIMIZATION")

        return quantized_aimodel

    def __staticQuantizationOnnx(self, model_name, device, inputs):
        """
        Apply Static Quantization with ONNX
        """  

        model_path = str(PROJECT_ROOT / "ModelData" / "ONNXModels" / f"{model_name}.onnx")
        model_prep_path = str(PROJECT_ROOT / "ModelData" / "ONNXModels" / f"{model_name}_prep.onnx")
        model_quantized_path = str(PROJECT_ROOT / "ModelData" / "ONNXModels" / f"{model_name}_quantized.onnx")

        onnx.shape_inference.infer_shapes_path(model_path, model_prep_path)

        quantization.shape_inference.quant_pre_process(
            model_prep_path, 
            model_prep_path, 
            skip_symbolic_shape=False
        )

        q_format = quantization.QuantFormat.QOperator

        q_static_opts = None

        if device == "gpu" or device == "cuda":
            q_static_opts = {
                "ActivationSymmetric": True,
                "WeighSymmetric": True
            }
        elif device == "cpu":
            q_static_opts = {
                "ActivationSymmetric": False,
                "WeightSymmetric": True
            }

        # Calibration dataset
        qdr = CustomCalibrationDataReader(inputs, input_name = "input")

        logger.info("Starting Static Quantization (QOperator)...")

        quantized_model = quantization.quantize_static(
            model_input=model_prep_path,
            model_output=model_quantized_path,
            calibration_data_reader=qdr,
            quant_format = q_format,
            per_channel=False,
            weight_type = QuantType.QInt8,
            activation_type=QuantType.QUInt8,
            extra_options=q_static_opts
        )

        logger.info("Quantization Complete")

        if quantized_model is not None:
            return True
        else:
            return False
   
class MissingAIModelError(Exception):
    """
    Raised when an optimization is applied before an AIModel is set.
    """

    pass

if __name__ == "__main__":



    # Model Example
    model_weights_path = PROJECT_ROOT / "ModelData" / "Weights"

    # Dataset Example
    data_dir = str(PROJECT_ROOT / "ModelData" / "Dataset" / "casting_data")
    dataset_info = {
        'data_dir': data_dir,
        'batch_size': 32
    }
    dataset = DataWrapper()



    # -------- TEST WITH BASE MODEL ---------------

    efficient_path = str(model_weights_path / "casting_efficientnet_b0.pth")

    # Faking user input
    efficient_info = {
        'module': 'torchvision.models',
        'model_name': "efficientnet_v2",
        'native': True,
        'distilled': False,
        'weights_path': efficient_path,
        'device': "cpu",
        'class_name': 'efficientnet_b0',
        'weights_class': 'EfficientNet_B0_Weights.DEFAULT',
        'image_size': 224,
        'num_classes': 2,
        'description': 'efficientnet_v2 from torchvision'
    }

    mobile_path = str(model_weights_path / "mobilenet_v2.pth")
    mobile_info = {
        'module': 'torchvision.models',
        'model_name': "mobilenet_v2",
        'native': True,
        'distilled': False,
        'weights_path': mobile_path,
        'device': "cpu",
        'class_name': 'mobilenet_v2',
        'weights_class': 'MobileNet_V2_Weights.DEFAULT',
        'image_size': 224,
        'num_classes': 2,
        'description': 'mobilenet_v2 from torchvision'

    }

    # Creating base model
    efficientnet = AIModel(efficient_info)
    logger.debug(f"Inference with {efficientnet.getInfo('model_name')}, description: {efficientnet.getInfo('description')}")
    logger.debug(f"Test inference on casting product")

    # Attaching dataset to unpruned model
    dataset.loadInferenceData(model_info = efficientnet.getAllInfo(), dataset_info = dataset_info)
    inference_loader = dataset.getLoader()

    # Inference with unpruned model
    efficientnet.createOnnxModel(inference_loader)
    efficientnet.runInference(inference_loader)

    # Base model for mobilenet
    mobilenet = AIModel(mobile_info)
    mobilenet_dataset = DataWrapper()
    mobilenet_dataset.loadInferenceData(model_info = mobile_info, dataset_info = dataset_info)
    mobilenet_loader = dataset.getLoader()
    mobilenet.createOnnxModel(mobilenet_loader)
    mobilenet.runInference(mobilenet_loader)

    # -----------------------------------------------

    # -------- TEST WITH PRUNED MODEL -------------------
    #Pruning Info example
    pruning_info = {
        "method": "Random",
        "n": 1,
        "amount": 0.2
    }

    # Creating Pruning Optimization Object
    pruning_optimizator = PruningOptimization(pruning_info)
    
    # Creating new Pruned AIModel
    pruning_optimizator.setAIModel(efficientnet)
    pruned_efficientnet = pruning_optimizator.applyOptimization(inference_loader)

    # Attaching dataset to pruned model
    dataset.loadInferenceData(model_info = pruned_efficientnet.getAllInfo(), dataset_info = dataset_info)
    inference_loader = dataset.getLoader()

    # Inference with prunde model
    pruned_efficientnet.createOnnxModel(inference_loader)
    pruned_efficientnet.runInference(inference_loader)

    # -----------------------------------------------

    # ---------- TEST WITH QUANTIZED MODEL ---------------

    quantization_info = {
        "arch": "x86", #aarch
        "method": "QInt8",
        "type": "static"
    }

    quantization_optimizator = QuantizationOptimization(quantization_info)
    
    # Creating new quantized AIModel
    quantization_optimizator.setAIModel(efficientnet)
    quantized_efficientnet = quantization_optimizator.applyOptimization(inference_loader)

    # Attaching dataset to quantized model
    dataset.loadInferenceData(model_info = quantized_efficientnet.getAllInfo(), dataset_info = dataset_info)
    inference_loader = dataset.getLoader()

    quantized_efficientnet.runInference(inference_loader)

    quantization_optimizator.setAIModel(mobilenet)
    quantized_mobilenet = quantization_optimizator.applyOptimization(mobilenet_loader)
    mobilenet_dataset.loadInferenceData(model_info = quantized_mobilenet.getAllInfo(), dataset_info = dataset_info)
    mobilenet_loader = mobilenet_dataset.getLoader()
    quantized_mobilenet.runInference(mobilenet_loader)


    # -----------------------------------------------



