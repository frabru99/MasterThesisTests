import torch
from torch import nn
import torch.nn.utils.prune as prune
import torch.nn.functional as F

#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = "cpu"


class pruningConfig:
    radomPruningAmount = 0.3 
    l1UnstructuredAmont=0.1
    lnStructuredAmount = 0.5
    lnStructuredn = 2
    globalAmount= 0.3


def simple_pruning_fn(model: nn.Module):

    # In this example function it will be pruned the first convolutional layer
    module = model.conv1

    # The firs parameter is the module which will be pruned
    # Amount stands for percentual of the connections that will be pruned
    # name is the type of parameter pruned
    prune.random_unstructured(module, name="weight", amount=pruningConfig.radomPruningAmount)

    prune.remove(module, name="weight")
    # The pruning process has removed the parameter "weight" and replaced it with a new parameter called weight_origin
    # It is still the unpruned version of the tensor, for now bias is not pruned so remains the same
    #print(list(module.named_parameters()))


    # The pruning mask generated by the pruning is saved as a module buffer named weight_mask (i.e appending "_mask to the initial parameter name")
    #print(list(module.named_buffers()))

    # For the forward pass to work without modification, the weight attribute needs to exist. The pruning tech. that was used compute the pruned version
    # of the weight (combining the mask with original parameter) and store them in the attribute weight para. Note, this is no longer a parameter of the module
    # it is now simply an attribute
    #print(module.weight)

    # PyTorch uses a technique to apply pruning mask dynamically, rather than permanently setting weights to zero, that's called reparameterization combined with hooks
    # the weight placeholder (its value is weight = weight_origin * weight_mask) is calculated before the module starts its actual forward+
    # This is done with a function called forward pre-hook
    #print(module._forward_pre_hooks)

    # For completeness we can prune also the bias, just for the sake of trying another pruning technique, here we prune the 3 smallest entries by L1 norm
    # L1 norm stands for absolute value, so this is used to choose the smallest entries
    prune.l1_unstructured(module, name="bias", amount=3)

    prune.remove(module, name="bias")



def iterative_pruning_fn(model: nn.Module):

    module = model.conv1
    prune.random_unstructured(module, name = "weight", amount = pruningConfig.radomPruningAmount)
    
    # Lets say we want to even further prune the weight parameter, this time with structured pruning, we can do it by using ln_structured()
    # Pruning along the 0th axis of the tensor (the output channels with a dimensionality of six for conv1) based on L2 norm (squared magnitude)
    prune.ln_structured(module, name="weight", amount=pruningConfig.lnStructuredAmount, n=2, dim=0)

    # The hook of structured and iterative pruning is of the type below and will store the history of pruning applied 
    # to the weight parameter
    for hook in module._forward_pre_hooks.values():
        if hook._tensor_name == "weight": # select out the correct hook
            break

    print(list(hook)) # pruning history in the container



def pruning_multiple_parameters(model: nn.Module):

    for name, module in model.named_modules():

        #prune 20% of connections in all 2D-conv layers
        if isinstance(module, torch.nn.Conv2d):
            prune.l1_unstructured(module, name="weight", amount=pruningConfig.l1UnstructuredAmont)
        
        #prune 40% of connections in all Fully connected layers
        elif isinstance(module, torch.nn.Linear):
            prune.l1_unstructured(module, name="weight", amount=0.4)

        #print(dict(model.named_buffers()).keys()) # To verify that all masks exist

def global_pruning(model: nn.Module):


    # Assuming 'model' is your torch model
    parameters_to_prune = []

    # Iterate over all named modules in the model
    for name, module in model.named_modules():
        # Only prune standard layers that have a 'weight' parameter
        if isinstance(module, (nn.Conv2d, nn.Linear)):
                    # Append the module and the parameter name ("weight") to the list
                parameters_to_prune.append((module, 'weight'))

    print(f"PARAMETERS TO PRUNE {len(parameters_to_prune)}")

    # We can do this by using global_unstructured
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=pruningConfig.globalAmount,
    )

    #print_sparsity_of_model(model)

    return parameters_to_prune


def remove_pruning_re_parametrization(model: nn.Module, parameters_to_prune):

    for module, parameter_name in parameters_to_prune:

        # To make the pruning permanente we can remove the re-parametrization in terms of weight_orig and weight_mask and remove the forward_pre_hook,
        # we can use the remove fn from torch.nn.utils.prune. Note that this doesn't undo the pruning, as if it never happened. It simply makes it permanent, 
        # instead, by reassingning the parameter weight to the model parameters in its pruned version
        
        prune.remove(module, parameter_name)

        #print(list(module.named_parameters()))
        #print(list(module.named_buffers()))


def print_sparsity_of_model(model: nn.Module):

    total_zeros = 0
    total_elements = 0
    for module, name in pruningConfig.parametersToPrune:

        weight = getattr(module, name)

        total_zeros += torch.sum(weight==0).item()

        total_elements +=  weight.nelement()
    

    print(
        "Global sparsity: {:.2f}%".format(
            100. * float(total_zeros)
            / float(total_elements)
        )
    )


