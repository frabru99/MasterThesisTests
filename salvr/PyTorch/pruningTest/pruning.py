import torch
from torch import nn
import torch.nn.utils.prune as prune
import torch.nn.functional as F
from model import LeNet

#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = "cpu"

def simple_pruning_fn(model: nn.Module):

    # In this example function it will be pruned the first convolutional layer
    module = model.conv1

    # The firs parameter is the module which will be pruned
    # Amount stands for percentual of the connections that will be pruned
    # name is the type of parameter pruned
    prune.random_unstructured(module, name="weight", amount=0.3)

    # The pruning process has removed the parameter "weight" and replaced it with a new parameter called weight_origin
    # It is still the unpruned version of the tensor, for now bias is not pruned so remains the same
    print(list(module.named_parameters()))


    # The pruning mask generated by the pruning is saved as a module buffer named weight_mask (i.e appending "_mask to the initial parameter name")
    print(list(module.named_buffers()))

    # For the forward pass to work without modification, the weight attribute needs to exist. The pruning tech. that was used compute the pruned version
    # of the weight (combining the mask with original parameter) and store them in the attribute weight para. Note, this is no longer a parameter of the module
    # it is now simply an attribute
    print(module.weight)

    # PyTorch uses a technique to apply pruning mask dynamically, rather than permanently setting weights to zero, that's called reparameterization combined with hooks
    # the weight placeholder (its value is weight = weight_origin * weight_mask) is calculated before the module starts its actual forward+
    # This is done with a function called forward pre-hook
    print(module._forward_pre_hooks)

    # For completeness we can prune also the bias, just for the sake of trying another pruning technique, here we prune the 3 smallest entries by L1 norm
    # L1 norm stands for absolute value, so this is used to choose the smallest entries
    prune.l1_unstructured(module, name="bias", amount=3)


def iterative_pruning_fn(model: nn.Module):

    module = model.conv1
    prune.random_unstructured(module, name = "weight", amount = 0.3)
    
    # Lets say we want to even further prune the weight parameter, this time with structured pruning, we can do it by using ln_structured()
    # Pruning along the 0th axis of the tensor (the output channels with a dimensionality of six for conv1) based on L2 norm (squared magnitude)
    prune.ln_structured(module, name="weight", amount=0.5, n=2, dim=0)

    # The hook of structured and iterative pruning is of the type below and will store the history of pruning applied 
    # to the weight parameter
    for hook in module._forward_pre_hooks.values():
        if hook._tensor_name == "weight": # select out the correct hook
            break

    print(list(hook)) # pruning history in the container

def serializing_a_pruned_model(model: nn.Module):

    # All relevant tensors, including the mask buffers and the original parameters used to compute the pruned tensors are stored in 
    # the modelâ€™s state_dict and can therefore be easily serialized and saved, if needed.
    print(model.state_dict().keys())

def remove_pruning_re_parametrization(model: nn.Module):

    module = model.conv1

    # To make the pruning permanente we can remove the re-parametrization in terms of weight_orig and weight_mask and remove the forward_pre_hook,
    # we can use the remove fn from torch.nn.utils.prune. Note that this doesn't undo the pruning, as if it never happened. It simply makes it permanent, 
    # instead, by reassingning the parameter weight to the model parameters in its pruned version
    prune.remove(module, "weight")
    print(list(module.named_parameters()))
    print(list(module.named_buffers()))

def pruning_multiple_parameters(model: nn.Module):

    for name, module in model.named_modules():

        #prune 20% of connections in all 2D-conv layers
        if isinstance(module, torch.nn.Conv2d):
            prune.l1_unstructured(module, name="weight", amount=0.2)
        
        #prune 40% of connections in all Fully connected layers
        elif isinstance(module, torch.nn.Linear):
            prune.l1_unstructured(module, name="weight", amount=0.4)

        #print(dict(model.named_buffers()).keys()) # To verify that all masks exist

def global_pruning(model: nn.Module):


    # We only looke, in the previous functions, at what is called as local pruning. However, a common and perhaps more powerful techniques is 
    # to prune the model all at once, by removing (for istance) the lowest 20% of connections across the whole model. 
    parameters_to_prune = (
        (model.conv1, 'weight'),
        (model.conv2, 'weight'),
        (model.fc1, 'weight'),
        (model.fc2, 'weight'),
        (model.fc3, 'weight'),
    )

    # We can do this by using global_unstructured
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=0.2,
    )

    print_sparsity_of_model(model)

def print_sparsity_of_model(model: nn.Module):

    print(
        "Sparsity in conv1.weight: {:.2f}%".format(
            100. * float(torch.sum(model.conv1.weight == 0))
            / float(model.conv1.weight.nelement())
        )
    )
    print(
        "Sparsity in conv2.weight: {:.2f}%".format(
            100. * float(torch.sum(model.conv2.weight == 0))
            / float(model.conv2.weight.nelement())
        )
    )
    print(
        "Sparsity in fc1.weight: {:.2f}%".format(
            100. * float(torch.sum(model.fc1.weight == 0))
            / float(model.fc1.weight.nelement())
        )
    )
    print(
        "Sparsity in fc2.weight: {:.2f}%".format(
            100. * float(torch.sum(model.fc2.weight == 0))
            / float(model.fc2.weight.nelement())
        )
    )
    print(
        "Sparsity in fc3.weight: {:.2f}%".format(
            100. * float(torch.sum(model.fc3.weight == 0))
            / float(model.fc3.weight.nelement())
        )
    )
    print(
        "Global sparsity: {:.2f}%".format(
            100. * float(
                torch.sum(model.conv1.weight == 0)
                + torch.sum(model.conv2.weight == 0)
                + torch.sum(model.fc1.weight == 0)
                + torch.sum(model.fc2.weight == 0)
                + torch.sum(model.fc3.weight == 0)
            )
            / float(
                model.conv1.weight.nelement()
                + model.conv2.weight.nelement()
                + model.fc1.weight.nelement()
                + model.fc2.weight.nelement()
                + model.fc3.weight.nelement()
            )
        )
    )


if __name__ == "__main__":

    model = LeNet().to(device=device)

    #simple_pruning_fn(model)

    iterative_pruning_fn(model)

    #pruning_multiple_parameters(model)

    #simple_pruning_fn(model)
    #remove_pruning_re_parametrization(model)

    #global_pruning(model)





