{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "70bPd60o5du3",
        "9iI1JAY9VF3k",
        "UfYC_JtPXLxq",
        "1Pn79CQ2aM8w",
        "Po4mOEtrfQuC",
        "2QwFGLM6ioDK",
        "edQ-dcjnlCJJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "DF22E2fGqwUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next version of Tensor Flow (actual 2.19.0, next 2.20.0) some tf.lite functions will be deprecated, so in this notebook there will be also the new versions of classes and functions."
      ],
      "metadata": {
        "id": "rvddo2LSYnmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The new module for edge-devices\n",
        "!python3 -m pip install ai-edge-litert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGH91noiY_tY",
        "outputId": "ce277835-81b7-44cd-8d7f-ac93c4d18fea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ai-edge-litert\n",
            "  Downloading ai_edge_litert-2.0.2-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting backports.strenum (from ai-edge-litert)\n",
            "  Downloading backports_strenum-1.2.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (25.9.23)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert) (4.15.0)\n",
            "Downloading ai_edge_litert-2.0.2-cp312-cp312-manylinux_2_27_x86_64.whl (91.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports_strenum-1.2.8-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: backports.strenum, ai-edge-litert\n",
            "Successfully installed ai-edge-litert-2.0.2 backports.strenum-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "egS99VaHqlg6"
      },
      "outputs": [],
      "source": [
        "# All depencies\n",
        "import os\n",
        "import numpy as np\n",
        "import h5py # A pythonic interface to the HDF5 binary data format (for model in keras)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.metrics import accuracy_score\n",
        "from ai_edge_litert.interpreter import Interpreter # The new interpreter\n",
        "from sys import getsizeof"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA0Qc7zAriZG",
        "outputId": "4fc2be61-f404-4093-abe9-08b3b187ac17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "aK7kwP2-sTnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_size(file_path):\n",
        "  \"\"\"\n",
        "  Helper function to return file size in bytes\n",
        "  \"\"\"\n",
        "  size = os.path.getsize(file_path)\n",
        "  return size"
      ],
      "metadata": {
        "id": "lfBW6L_Asa39"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bytes(size, unit=None):\n",
        "  \"\"\"\n",
        "  Helper function to return file size in different units\n",
        "  \"\"\"\n",
        "  if unit == \"KB\":\n",
        "    return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
        "  elif unit == \"MB\":\n",
        "    return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
        "  else:\n",
        "    return print('File size: ' + str(size) + ' bytes')"
      ],
      "metadata": {
        "id": "daP_s_ZQss0v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Fashion MNIST Dataset"
      ],
      "metadata": {
        "id": "U0b6IWeht3DV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "CtwNltBSx-c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fashion mnist dataset, dividing it into train and test sets\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcAt25Q3t8p1",
        "outputId": "1e1f6bbc-90a5-48fc-a48a-ee5675bf2904"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
        "               'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "metadata": {
        "id": "Zghy72CA2YE_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore Training data\n"
      ],
      "metadata": {
        "id": "m_XKAaQp2SvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The images are numpy 3d (number of images, x_dim, y_dim) dimensional arrays\n",
        "print(type(train_images))\n",
        "print(train_images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqVbawy52uIy",
        "outputId": "f9beea7b-0a12-4072-bfcb-b774c8b1b696"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(60000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiA-9RL921fK",
        "outputId": "50512585-46c4-4fe0-d0f0-902ef8d29347"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if there are ten unique classes in the array labels\n",
        "np.unique(train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsadI3OK3mhl",
        "outputId": "17f47b21-d10c-4c72-8657-040e3e551ea2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore Test Dataset"
      ],
      "metadata": {
        "id": "mvJq3hH033YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(test_images))\n",
        "print(test_images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSeDjG0632R5",
        "outputId": "97fc9b5f-c53e-49a6-94a8-848a1b9fdfad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbKh8VFU4HqU",
        "outputId": "8b2a5f96-bc1a-4dbe-d3d5-f2d2d5e55574"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proprocessing"
      ],
      "metadata": {
        "id": "6hi_knTg4K8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0], cmap=\"gray\")\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "iwEWLh894N4j",
        "outputId": "143243a2-6c0c-40a9-8ecb-e6a08a6e9de1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing to scale the values of images from [0,255] to [0,1]\n",
        "# To stabilize the learning process (a good practice)\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "wnjMl-fP4eAi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build & Compile the model"
      ],
      "metadata": {
        "id": "70bPd60o5du3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple NN to classify the images\n",
        "model = keras.Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10) # Last layer of ten classes\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlCuaLM55bOu",
        "outputId": "61fdc73f-d1ce-4b9f-fbfd-39ca7ea67002"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "c5gwacKy6e8e",
        "outputId": "6e0dca8c-0548-4928-e0ba-9cbfb777b8d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with adam optimizator\n",
        "model.compile(optimizer='adam',\n",
        "              loss = SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy']\n",
        "              )"
      ],
      "metadata": {
        "id": "ZtzlUdoQ5_WP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The fit/train function starts the training of the models\n",
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqht5Q1U6n7N",
        "outputId": "0b47e6af-cebb-46cd-87ca-99670b5b891f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.7770 - loss: 0.6357\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8604 - loss: 0.3879\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8767 - loss: 0.3387\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8819 - loss: 0.3156\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8884 - loss: 0.3019\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8935 - loss: 0.2833\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9008 - loss: 0.2652\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9027 - loss: 0.2612\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9086 - loss: 0.2476\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9124 - loss: 0.2358\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c337b5a8bf0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model reference and name\n",
        "KERAS_MODEL_NAME = 'tf_model_fashion_mnist.h5'"
      ],
      "metadata": {
        "id": "Y-2IBqkQ7LHe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model, this will create a h5 file\n",
        "model.save(KERAS_MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfVJ-JXkS16E",
        "outputId": "b7423eca-e7fe-410a-a9bc-9dcda8eee633"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_bytes(get_file_size(KERAS_MODEL_NAME), \"MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pufG7gWS-Z6",
        "outputId": "e98e687c-8706-4f95-a5c6-a8b490971af1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 1.187 Megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras_model_size = get_file_size(KERAS_MODEL_NAME)"
      ],
      "metadata": {
        "id": "IJpULCgbTcnM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print('\\nTest accuracy is {}%'.format(round(100 * test_acc, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TGOUSiBTjEd",
        "outputId": "e4859727-1a56-4402-a8c9-97621d2e937b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - 2ms/step - accuracy: 0.8800 - loss: 0.3471\n",
            "\n",
            "Test accuracy is 88.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF Lite Model"
      ],
      "metadata": {
        "id": "9iI1JAY9VF3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model name and reference but for the tflite model version\n",
        "TF_LITE_MODEL_FILE_NAME = 'tf_lite_model.tflite'"
      ],
      "metadata": {
        "id": "_3dpXQHaVInD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Converter from module tf.lite\n",
        "# then converting the model\n",
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = tf_lite_converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An4juEE6VO87",
        "outputId": "86e1bc1e-dc90-4fb2-b86d-a0b71049ed36"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpgu_i3xlg'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136560548846800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model in a tflite file, showing the byte size\n",
        "open(TF_LITE_MODEL_FILE_NAME, 'wb').write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwLlGrQNVywI",
        "outputId": "f0ae54fa-d148-495b-9e08-967bb876c879"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "408812"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As it is possible to see from this code, the file is smaller than the h5 file\n",
        "convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVU8SyzkWgUA",
        "outputId": "e8f1bff7-ef29-4e30-ccc8-70ee3c24b9bd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 399.23 Kilobytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_lite_file_size = get_file_size(TF_LITE_MODEL_FILE_NAME)"
      ],
      "metadata": {
        "id": "MrDnHyJDW0V2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Input Tensor Shape"
      ],
      "metadata": {
        "id": "UfYC_JtPXLxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check how the shapes are changed\n",
        "# It's necessary to create a tflite Interpreter to read correctly a tflite file\n",
        "# interpreter = tf.lite.Interpreter(model_path=TF_LITE_MODEL_FILE_NAME)\n",
        "new_interpreter = Interpreter(model_path=TF_LITE_MODEL_FILE_NAME) #new interpreter from ai_edge module\n",
        "input_details = new_interpreter.get_input_details()\n",
        "output_details = new_interpreter.get_output_details()\n",
        "print('Input Shape:', input_details[0]['shape'])\n",
        "print('Input Type:', input_details[0]['dtype'])\n",
        "print('Output Shape:', output_details[0]['shape'])\n",
        "print('Output Shape:', output_details[0]['dtype'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpfba7GyXQs5",
        "outputId": "b09cd319-510b-4c7a-9878-71e44a011913"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: [ 1 28 28]\n",
            "Input Type: <class 'numpy.float32'>\n",
            "Output Shape: [ 1 10]\n",
            "Output Shape: <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resize Tensor Shape"
      ],
      "metadata": {
        "id": "1Pn79CQ2aM8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As seen before the Input Shape accepts only one input at time\n",
        "# Reshape the input and output tensor for a batch inference (without the loop)\n",
        "new_interpreter.resize_tensor_input(input_details[0]['index'], (10000, 28, 28))\n",
        "new_interpreter.resize_tensor_input(output_details[0]['index'], (10000, 10))\n",
        "new_interpreter.allocate_tensors()\n",
        "input_details = new_interpreter.get_input_details()\n",
        "output_details = new_interpreter.get_output_details()\n",
        "print('Input Shape:', input_details[0]['shape'])\n",
        "print('Input Type:', input_details[0]['dtype'])\n",
        "print('Output Shape:', output_details[0]['shape'])\n",
        "print('Output Shape:', output_details[0]['dtype'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7KeQQnRaP3S",
        "outputId": "b621c816-676e-4592-de0e-11dbcdacf5b9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: [10000    28    28]\n",
            "Input Type: <class 'numpy.float32'>\n",
            "Output Shape: [10000    10]\n",
            "Output Shape: <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another problem, the model now accepts float32\n",
        "# Test images are float64\n",
        "print(test_images.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-tosXj6cg9w",
        "outputId": "b6338297-2b08-474c-f7f2-c3172387053e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the test images size\n",
        "test_images_numpy = np.array(test_images, dtype=np.float32)\n",
        "print(test_images_numpy.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYJVMvS4c5a-",
        "outputId": "781caa7a-107d-470e-a379-17e9a3d66e60"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now it's possible to evaluate the tflite model\n",
        "new_interpreter.set_tensor(input_details[0]['index'], test_images_numpy)\n",
        "new_interpreter.invoke() # With this function the interpreter creates the outputs\n",
        "tflite_model_predictions = new_interpreter.get_tensor(output_details[0]['index'])\n",
        "print('Predictions results shape:', tflite_model_predictions.shape)\n",
        "prediction_classes = np.argmax(tflite_model_predictions, axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQWGjGTrdA1R",
        "outputId": "950ad872-f776-4b32-94c0-e88451229a0a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions results shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each of 10000 images we have one class predicted\n",
        "acc = accuracy_score(prediction_classes, test_labels)\n",
        "print(f'Test accuracy TFLITE model is {round(100*acc, 2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBDo3hvGeKjW",
        "outputId": "af083f18-7e8a-4591-f042-55cf6cd0460f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy TFLITE model is 88.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The accuracy is pratically the same, but the model size is changed\n",
        "# the tflite file is a third of the original\n",
        "tf_lite_file_size/keras_model_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8in7FQcHepqX",
        "outputId": "1b71bf30-a486-4a14-97ee-be5d58d54e19"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3285044806438756"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF Lite Model Float 16"
      ],
      "metadata": {
        "id": "Po4mOEtrfQuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TF_LITE_MODEL_FLOAT_16_FILE_NAME = \"tf_lite_float_16_model.tflite\"\n"
      ],
      "metadata": {
        "id": "U694xM9RglWk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We haven't seen how a converter can be configurated\n",
        "# For instance here we want to create a 16 float tflite converter\n",
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tf_lite_converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = tf_lite_converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YJuwf-lhfL8",
        "outputId": "753837dc-21a0-469b-91db-8dccf185bbd2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpmnth8gwj'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136560548846800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open(TF_LITE_MODEL_FLOAT_16_FILE_NAME, 'wb').write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSaR-q91iI0n",
        "outputId": "347edc62-f351-45cc-f03a-57c74d8849f0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205808"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_bytes(get_file_size(TF_LITE_MODEL_FLOAT_16_FILE_NAME), \"KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7o3_N6iPBL",
        "outputId": "70993d58-089e-4083-b2f4-b835d8bc9d50"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 200.984 Kilobytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_lite_float_16_file_size = get_file_size(TF_LITE_MODEL_FLOAT_16_FILE_NAME)"
      ],
      "metadata": {
        "id": "H1eQzShJiaMR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_lite_float_16_file_size / keras_model_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HryycGJkiiV1",
        "outputId": "d713db42-57a8-4ef8-c884-47116d0e77d4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1653788297612466"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF Lite Size Quantized"
      ],
      "metadata": {
        "id": "2QwFGLM6ioDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the following quantization we will reduce the dtype of weights from float32 to int8"
      ],
      "metadata": {
        "id": "QwHQFaw1mMcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TF_LITE_SIZE_QUANT_MODEL_FILE_NAME = 'tf_lite_quant_model.tflite'"
      ],
      "metadata": {
        "id": "0VbzDhGbi1Zl"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Optimization: OPTIMIZE_FOR_SIZE\n",
        "This setting enables weight-only quantization.\n",
        "\n",
        "What it does: It scans the model and converts all internal weights (the constant tensors, like the weights in a Dense layer) from 32-bit floating-point numbers (float32) to 8-bit integers (int8).\n",
        "\n",
        "Main Benefit: The primary goal is to reduce the model's final file size by approximately 4x. This is excellent for saving storage space (ROM) on a device.\n",
        "\n",
        "Why Input/Output is still float32: This optimization creates a \"hybrid\" model. The inputs and outputs remain float32. During runtime, the TFLite Interpreter performs \"de-quantization\" on the fly—it converts the int8 weights back to float32 before executing the calculations.\n",
        "\n",
        "Performance Impact:\n",
        "\n",
        "✅ Storage (ROM): Drastically reduced.\n",
        "\n",
        "❌ Speed (Latency) & Runtime RAM: No significant improvement. Since all calculations are still performed using float32 math, we don't get the speed and memory benefits of an integer-only pipeline.\n",
        "\n",
        "(To gain speed and reduce RAM, one would need to use Full Integer Quantization, which also converts the inputs, outputs, and intermediate math to int8 but requires a calibration dataset)."
      ],
      "metadata": {
        "id": "LVI25FA1mg8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# WARNING: There are also optimizations for latency\n",
        "tf_lite_converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_model = tf_lite_converter.convert()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X-6K7XYWjGE2",
        "outputId": "3b699e07-450a-4342-8470-841ad602f776"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpsnbrlstd'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136560548846800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open(TF_LITE_SIZE_QUANT_MODEL_FILE_NAME, 'wb').write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPyH2UbJj7hv",
        "outputId": "13441839-280f-46a1-e07d-f8e71cee3ef2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105672"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_bytes(get_file_size(TF_LITE_SIZE_QUANT_MODEL_FILE_NAME), \"KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSUklL1vj-16",
        "outputId": "b4e741b1-fff3-4a24-931d-8ba710c4b2c1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 103.195 Kilobytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_lite_float_quant_file_size = get_file_size(TF_LITE_SIZE_QUANT_MODEL_FILE_NAME)\n"
      ],
      "metadata": {
        "id": "7mSxFNPUkjUf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_lite_float_quant_file_size / keras_model_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm9GGpsnkqXm",
        "outputId": "f34d2fc4-356e-4fe4-b89e-f8bdc901a91a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08491366564239705"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Input Quantized Tensor Shape"
      ],
      "metadata": {
        "id": "edQ-dcjnlCJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As anticipated the input and output dtype are still float32, this means that the model dequantizes the weights to float32 during inference time.\n",
        "\n",
        "For this it isn't necessary to show again the accuracy because is the same as before."
      ],
      "metadata": {
        "id": "Cx_HlznPmwXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = Interpreter(model_path=TF_LITE_SIZE_QUANT_MODEL_FILE_NAME)\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print('Input Shape:', input_details[0]['shape'])\n",
        "print('Input Type:', input_details[0]['dtype'])\n",
        "print('Output Shape:', output_details[0]['shape'])\n",
        "print('Output Shape:', output_details[0]['dtype'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTCPOMmlkxIs",
        "outputId": "b1a9f6cb-91bc-4041-84c1-2896a1dcb6f6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: [ 1 28 28]\n",
            "Input Type: <class 'numpy.float32'>\n",
            "Output Shape: [ 1 10]\n",
            "Output Shape: <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calibration Dataset"
      ],
      "metadata": {
        "id": "j8rNtWKPpuJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a calibration dataset is important to get a **representative set** of data which the model will see. It's not a problem to take a small number of data from the training set because the purpose of quantization is to map floa32 to int8 with minimum loss of information, it's nothing correlated to the model accuracy (Post Training Quantization)."
      ],
      "metadata": {
        "id": "jSykqAr5qbV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The TFLite Converter needs a representative generation function\n",
        "# We will use the yield python keyword, this transform a function in a generator\n",
        "# It's like a return for the first cycle, then the function remember the index\n",
        "def representative_data_gen():\n",
        "  for i in range(100):\n",
        "    image = train_images[i]\n",
        "\n",
        "    # It's necessary to add a batch dimension for compatibility issues with the model\n",
        "    image = np.expand_dims(image, axis=0).astype(np.float32)\n",
        "\n",
        "    yield [image]\n"
      ],
      "metadata": {
        "id": "yc0F4EvppwSQ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Integer Quantization"
      ],
      "metadata": {
        "id": "Z8VGEFuZo-Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TF_LITE_FULL_INT_QUANT_MODEL_FILE_NAME = 'tf_lite_full_int_quant_model.tflite'"
      ],
      "metadata": {
        "id": "flE8psHFpByN"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can create a quantized model\n",
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tf_lite_converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Forcing the model to only use integer math\n",
        "tf_lite_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "tf_lite_converter.inference_input_type = tf.uint8\n",
        "tf_lite_converter.inference_output_type = tf.uint8\n",
        "\n",
        "# Model FIQ Generation\n",
        "tflite_model = tf_lite_converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_8SCsImpRHi",
        "outputId": "5eb99c43-48e4-462b-c9ad-add6fbab94df"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp0dsbogby'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136560548846800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136560548847568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open(TF_LITE_FULL_INT_QUANT_MODEL_FILE_NAME, 'wb').write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9c9O-6fxTsy",
        "outputId": "a256b0d4-f5c1-4902-c37f-4564240ee80d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107896"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = Interpreter(model_path=TF_LITE_FULL_INT_QUANT_MODEL_FILE_NAME)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print('Input Shape:', input_details[0]['shape'])\n",
        "print('Input Type:', input_details[0]['dtype'])\n",
        "print('Output Shape:', output_details[0]['shape'])\n",
        "print('Output Shape:', output_details[0]['dtype'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hYbkgcMxY2a",
        "outputId": "78694a47-13dc-4230-db9c-31a45c9ccffa"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: [ 1 28 28]\n",
            "Input Type: <class 'numpy.uint8'>\n",
            "Output Shape: [ 1 10]\n",
            "Output Shape: <class 'numpy.uint8'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To correctly rescale and quantize test images we have to use the new scale learnt from the calibration dataset. Then we can convert to uint8"
      ],
      "metadata": {
        "id": "-WzfCHhM4m2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_scale, input_zero_point = input_details[0]['quantization']\n",
        "print(f\"Scala di Input: {input_scale}\")\n",
        "print(f\"Zero-Point di Input: {input_zero_point}\")\n",
        "\n",
        "# 'test_images' qui sono i tuoi dati float32 normalizzati [0, 1]\n",
        "test_images_quantized_correctly = (test_images / input_scale) + input_zero_point\n",
        "\n",
        "# Ora puoi convertire in uint8\n",
        "test_images_quantized_correctly = test_images_quantized_correctly.astype(np.uint8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWec1nwYyapM",
        "outputId": "0d374736-0aa0-4e11-e55d-aff40b6d18e0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scala di Input: 0.003921568859368563\n",
            "Zero-Point di Input: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter.resize_tensor_input(input_details[0]['index'], (10000, 28, 28))\n",
        "interpreter.resize_tensor_input(output_details[0]['index'], (10000, 10))\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print('Input Shape:', input_details[0]['shape'])\n",
        "print('Input Type:', input_details[0]['dtype'])\n",
        "print('Output Shape:', output_details[0]['shape'])\n",
        "print('Output Shape:', output_details[0]['dtype'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_caLlEWyXPz",
        "outputId": "e9cbe700-4bd6-43fc-f897-7a43e7be7ae5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: [10000    28    28]\n",
            "Input Type: <class 'numpy.uint8'>\n",
            "Output Shape: [10000    10]\n",
            "Output Shape: <class 'numpy.uint8'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter.set_tensor(input_details[0]['index'], test_images_quantized_correctly)\n",
        "interpreter.invoke() # With this function the interpreter creates the outputs\n",
        "tflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])\n",
        "print('Predictions results shape:', tflite_model_predictions.shape)\n",
        "prediction_classes = np.argmax(tflite_model_predictions, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dP4nHKnytbE",
        "outputId": "9f69b82c-150a-4c6d-b187-541c5053ae4b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions results shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The accuracy is a bit lower but it is normal\n",
        "acc = accuracy_score(prediction_classes, test_labels)\n",
        "print(f'Test accuracy TFLITE model is {round(100*acc, 2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uro8z-2y1TCx",
        "outputId": "24bcd1ff-ed6a-4041-ff67-4c4cbf4283f4"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy TFLITE model is 88.37\n"
          ]
        }
      ]
    }
  ]
}